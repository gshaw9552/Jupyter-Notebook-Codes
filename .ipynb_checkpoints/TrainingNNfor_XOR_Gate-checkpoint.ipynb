{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b5f869-e07b-4f73-83fd-e51ad0fcde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708e5edf-7c5d-4390-b951-a70b5f766cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR gate inputs and outputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f27674-fd8e-4f70-840b-2a2de2ecac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c167770-ab4b-4399-a1a7-0b06fe9b3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases randomly\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "# Weights and biases for hidden and output layers\n",
    "weights_input_hidden = np.random.rand(input_layer_neurons, hidden_layer_neurons)\n",
    "weights_hidden_output = np.random.rand(hidden_layer_neurons, output_neurons)\n",
    "bias_hidden = np.random.rand(1, hidden_layer_neurons)\n",
    "bias_output = np.random.rand(1, output_neurons)\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0c67d6-b052-4b5e-a762-373f604a9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.1439874106607125\n",
      "Epoch 1000 Loss: 0.12471664883271598\n",
      "Epoch 2000 Loss: 0.12283768573557613\n",
      "Epoch 3000 Loss: 0.10998120920789847\n",
      "Epoch 4000 Loss: 0.08109962272100707\n",
      "Epoch 5000 Loss: 0.026354437895730574\n",
      "Epoch 6000 Loss: 0.008463006210208335\n",
      "Epoch 7000 Loss: 0.004458892657099934\n",
      "Epoch 8000 Loss: 0.0029222733318466274\n",
      "Epoch 9000 Loss: 0.002140895011829794\n"
     ]
    }
   ],
   "source": [
    "# Training the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    hidden_layer_activation = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "    \n",
    "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = (1/2) * (predicted_output - outputs) ** 2\n",
    "    \n",
    "    # Backpropagation\n",
    "    error = predicted_output - outputs\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_hidden_output -= learning_rate * hidden_layer_output.T.dot(d_predicted_output)\n",
    "    bias_output -= learning_rate * np.sum(d_predicted_output, axis=0, keepdims=True)\n",
    "    \n",
    "    weights_input_hidden -= learning_rate * inputs.T.dot(d_hidden_layer)\n",
    "    bias_hidden -= learning_rate * np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch} Loss: {np.mean(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a169f99c-ac79-4b82-bbc2-b6321a0b1ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training (Input to Hidden): [[3.6991303  5.699086  ]\n",
      " [3.70579679 5.73069504]]\n",
      "Bias after training (Hidden): [[-5.67051588 -2.37581163]]\n",
      "Weights after training (Hidden to Output): [[-8.02663537]\n",
      " [ 7.42116082]]\n",
      "Bias after training (Output): [[-3.35000977]]\n",
      "Final predictions: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print final weights, biases, and predictions\n",
    "print(\"Weights after training (Input to Hidden):\", weights_input_hidden)\n",
    "print(\"Bias after training (Hidden):\", bias_hidden)\n",
    "print(\"Weights after training (Hidden to Output):\", weights_hidden_output)\n",
    "print(\"Bias after training (Output):\", bias_output)\n",
    "\n",
    "# Final predictions\n",
    "final_predictions = sigmoid(np.dot(sigmoid(np.dot(inputs, weights_input_hidden) + bias_hidden), weights_hidden_output) + bias_output)\n",
    "print(\"Final predictions:\", final_predictions.round())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
